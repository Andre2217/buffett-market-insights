# ğŸ§  Projeto PrÃ¡tico â€” Web Mining & Crawler Scraping

## Pipeline AnalÃ­tico sobre o PortfÃ³lio da Berkshire Hathaway

---

### ğŸ“˜ Tema
AnÃ¡lise das **movimentaÃ§Ãµes de portfÃ³lio da Berkshire Hathaway** e do impacto das decisÃµes de **Warren Buffett** no mercado financeiro.

O projeto desenvolve um pipeline completo de **coleta, transformaÃ§Ã£o, carga e anÃ¡lise (ETL + EDA)** de dados obtidos via **Web Scraping** e **API**, permitindo explorar o comportamento dos investimentos da Berkshire e identificar tendÃªncias nas decisÃµes de alocaÃ§Ã£o de capital.

---

## ğŸ¯ Objetivo Geral

Construir um **pipeline ETL analÃ­tico completo** que:

- Coleta dados de **trÃªs fontes distintas** (duas via scraping e uma via API);
- Realiza o **tratamento, limpeza e padronizaÃ§Ã£o** dos dados obtidos;
- Gera arquivos estruturados em formato **Parquet**, simulando um **banco analÃ­tico local**;
- Permite anÃ¡lises financeiras e textuais com **Python e Jupyter Notebook**.

---

## âš™ï¸ Tecnologias e Ferramentas Utilizadas

- **Linguagem:** Python 3.9+
- **Ambiente:** VSCode com Jupyter Notebook (`.ipynb`)
- **Principais bibliotecas:**
  - `requests`, `BeautifulSoup`, `selenium` â€” Web Scraping
  - `pandas`, `numpy`, `pyarrow` â€” manipulaÃ§Ã£o e armazenamento de dados
  - `matplotlib`, `seaborn` â€” visualizaÃ§Ã£o de dados
  - `nltk`, `re`, `collections.Counter` â€” processamento de texto (anÃ¡lises de notÃ­cias)
  - `dotenv` â€” gerenciamento de variÃ¡veis sensÃ­veis (API Keys)

- **Formato dos dados tratados:** `.parquet`  
- **SaÃ­das geradas:** `.csv` (tabelas) e `.png` (grÃ¡ficos)

---

## ğŸ§© Estrutura do Pipeline ETL

### 1. Coleta de Dados
- **1.1 SÃ©ries histÃ³ricas de preÃ§os** via API (*Alpha Vantage*).  
- **1.2 MovimentaÃ§Ãµes do portfÃ³lio da Berkshire Hathaway** via Web Scraping.  
- **1.3 NotÃ­cias e manchetes sobre Warren Buffett e empresas relacionadas** via Web Scraping.

### 2. TransformaÃ§Ã£o
- Limpeza de dados, remoÃ§Ã£o de duplicidades, tratamento de colunas e padronizaÃ§Ã£o de formatos.

### 3. Carga
- ExportaÃ§Ã£o dos dados tratados para arquivos `.parquet` na pasta `data_clean/`.

### 4. AnÃ¡lise
- AnÃ¡lises exploratÃ³rias e visuais realizadas no notebook `AnaliseDadosWarrenBuffett.ipynb`.

---

## ğŸ“Š Notebook de AnÃ¡lise de Dados

O notebook **`AnaliseDadosWarrenBuffett.ipynb`** contÃ©m as anÃ¡lises exploratÃ³rias realizadas a partir dos dados consolidados no pipeline ETL.  
Ele gera grÃ¡ficos e tabelas salvos automaticamente em `outputs/outputsAnalise/`.

### Principais anÃ¡lises realizadas:
1. **Volume mÃ©dio diÃ¡rio por empresa**  
2. **EvoluÃ§Ã£o dos preÃ§os de fechamento ao longo do tempo**  
3. **Top 10 posiÃ§Ãµes mais valiosas do portfÃ³lio**  
4. **DiferenÃ§a percentual entre portfÃ³lio atual e anterior**  
5. **DistribuiÃ§Ã£o das variaÃ§Ãµes percentuais das posiÃ§Ãµes**  
6. **AÃ§Ãµes com maior frequÃªncia de mudanÃ§as**  
7. **Palavras mais frequentes nos tÃ­tulos das notÃ­cias**  
8. **Quantidade de notÃ­cias por termo pesquisado**

### SaÃ­das geradas automaticamente:
- ğŸ“ˆ GrÃ¡ficos: `outputs/outputsAnalise/graficos/`  
- ğŸ“‹ Tabelas: `outputs/outputsAnalise/tabelas/`  

Cada anÃ¡lise Ã© documentada dentro do notebook com explicaÃ§Ãµes e objetivos em markdown.

---

## ğŸ§¾ Arquivo `requirements.txt`

O arquivo **`requirements.txt`** lista todas as dependÃªncias do projeto, garantindo reprodutibilidade.  
Para instalar:

```bash
pip install -r requirements.txt
```

Exemplo de dependÃªncias incluÃ­das:
`
pandas
numpy
matplotlib
seaborn
requests
beautifulsoup4
selenium
pyarrow
nltk
python-dotenv
`

## ğŸš€ Como Executar o Projeto

### 1. Clone o repositÃ³rio:
   `git clone https://github.com/seu-usuario/seu-repositorio.git`
### 2. Configure sua chave da API Alpha Vantage:
Crie um arquivo .env na raiz do projeto e adicione:
  `ALPHA_VANTAGE_API_KEY=your_api_key_here`
### 3. Execute os notebooks na ordem:
  - `ColetaDadosWarrenBuffett.ipynb`
  - `AnaliseDadosWarrenBuffett.ipynb`

## ğŸ“‚ Estrutura de Pastas
`
/project-root
â”‚
â”œâ”€â”€ data_raw/           # Dados brutos coletados
â”œâ”€â”€ data_clean/         # Dados tratados e padronizados (.parquet)
â”œâ”€â”€ logs/               # Registros de execuÃ§Ã£o
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ outputsAnalise/
â”‚       â”œâ”€â”€ tabelas/    # Tabelas CSV das anÃ¡lises
â”‚       â””â”€â”€ graficos/   # GrÃ¡ficos gerados (.png)
â”‚
â”œâ”€â”€ ColetaDadosWarrenBuffett.ipynb       # Pipeline de coleta e tratamento
â”œâ”€â”€ AnaliseDadosWarrenBuffett.ipynb      # Notebook de anÃ¡lise exploratÃ³ria
â”œâ”€â”€ requirements.txt                     # DependÃªncias do projeto
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
`
## âš ï¸ Boas PrÃ¡ticas e ObservaÃ§Ãµes

  - ğŸ” Nunca compartilhe chaves de API ou credenciais no cÃ³digo.

  - ğŸ§¹ Os diretÃ³rios logs/ e outputs/ estÃ£o no .gitignore para evitar arquivos locais no versionamento.

  - ğŸ§  O projeto foi desenvolvido com fins educacionais e analÃ­ticos, simulando um pipeline real de Web Mining.

## ğŸ’¡ PossÃ­veis ExtensÃµes Futuras

  - IntegraÃ§Ã£o com Power BI ou Tableau para dashboards interativos.

  - ImplementaÃ§Ã£o de anÃ¡lise de sentimento nas notÃ­cias usando NLP.

  - InclusÃ£o de novas fontes de dados (ex: Yahoo Finance, CNBC, Bloomberg).

  - ConstruÃ§Ã£o de um pipeline automatizado com Airflow, Prefect ou Dagster.

  - CriaÃ§Ã£o de um banco de dados SQL Server ou DuckDB para consultas analÃ­ticas.

  - Modelos de correlaÃ§Ã£o entre movimentaÃ§Ãµes de portfÃ³lio e desempenho das aÃ§Ãµes.

  - AnÃ¡lise temporal de notÃ­cias para identificar reaÃ§Ã£o do mercado Ã s decisÃµes da Berkshire.

  - Painel web com Streamlit para visualizaÃ§Ã£o interativa dos resultados.

## ğŸ ConclusÃ£o

Este projeto demonstra o ciclo completo de um pipeline analÃ­tico em Python, desde a coleta automatizada de dados atÃ© a anÃ¡lise exploratÃ³ria e visualizaÃ§Ã£o.
Ele serve como base sÃ³lida para estudos em engenharia de dados, web scraping, anÃ¡lise financeira e NLP â€” podendo ser facilmente expandido para aplicaÃ§Ãµes reais de monitoramento de portfÃ³lio e notÃ­cias de mercado.
