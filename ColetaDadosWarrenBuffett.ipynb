{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432d207a",
   "metadata": {},
   "source": [
    "# Projeto Pr√°tico ‚Äî Web Mining & Crawler Scraping\n",
    "## Pipeline de Web Scraping para Banco Anal√≠tico Financeiro\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Tema: Movimenta√ß√µes da Berkshire Hathaway e o impacto das decis√µes de Warren Buffett no mercado financeiro\n",
    "\n",
    "Este projeto teve como objetivo desenvolver um pipeline de **coleta, transforma√ß√£o e carga (ETL)** voltado √† an√°lise de informa√ß√µes sobre as **movimenta√ß√µes de portf√≥lio da Berkshire Hathaway**, conglomerado de investimentos liderado por **Warren Buffett**, e o impacto dessas decis√µes no comportamento de mercado.\n",
    "\n",
    "A escolha desse tema se justifica por sua relev√¢ncia e atualidade: Warren Buffett √© amplamente reconhecido como um dos maiores investidores do mundo, e suas decis√µes frequentemente influenciam o valor das empresas nas quais investe ou vende participa√ß√£o. Assim, o estudo dessas movimenta√ß√µes, aliado a dados hist√≥ricos de mercado e not√≠cias relacionadas, permite observar padr√µes e rea√ß√µes econ√¥micas reais.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Objetivo Geral\n",
    "\n",
    "Construir um **pipeline ETL completo** que:\n",
    "- Coleta dados de **tr√™s fontes distintas** (duas via scraping e uma via API);\n",
    "- Realiza o tratamento e padroniza√ß√£o dos dados obtidos;\n",
    "- Gera arquivos estruturados no formato **Parquet**, simulando um **banco anal√≠tico local**;\n",
    "- Permite a explora√ß√£o futura dos dados em ferramentas SQL e dashboards anal√≠ticos.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Ferramentas e Tecnologias Utilizadas\n",
    "\n",
    "- **Linguagem:** Python 3.9+\n",
    "- **Ambiente de desenvolvimento:** VSCode com extens√£o Jupyter Notebook (`.ipynb`)\n",
    "- **Bibliotecas principais:**\n",
    "  - `requests` e `BeautifulSoup` para Web Scraping\n",
    "  - `selenium` para scraping din√¢mico (se necess√°rio)\n",
    "  - `pandas` e `pyarrow` para manipula√ß√£o e exporta√ß√£o de dados\n",
    "  - `datetime` e `os` para controle de execu√ß√£o e estrutura√ß√£o de arquivos\n",
    "\n",
    "- **Formato de sa√≠da dos dados:** `.parquet`\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Estrutura Geral do Pipeline ETL\n",
    "\n",
    "1. **Coleta de Dados**\n",
    "   - 1.1 S√©ries hist√≥ricas (via API ‚Äî *Alpha vantage*)\n",
    "   - 1.2 Movimenta√ß√µes da Berkshire Hathaway (via Web Scraping)\n",
    "   - 1.3 Not√≠cias sobre Warren Buffett e empresas relacionadas (via Web Scraping)\n",
    "2. **Transforma√ß√£o de Dados**\n",
    "   - Padroniza√ß√£o, limpeza, deduplica√ß√£o e ajustes de tipos.\n",
    "3. **Carga de Dados**\n",
    "   - Exporta√ß√£o dos resultados tratados para arquivos `.parquet`.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Coleta de Dados\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Sistema de Logs\n",
    "\n",
    "M√≥dulo respons√°vel por registrar eventos e informa√ß√µes importantes do sistema, facilitando o monitoramento e a depura√ß√£o durante a execu√ß√£o do c√≥digo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68becd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Criar pastas logs/ e outputs/ se n√£o existirem\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "log_filename = datetime.now().strftime(\"logs/log_%Y-%m-%d_%H-%M-%S.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ],\n",
    "     force=True\n",
    ")\n",
    "\n",
    "# Classe para redirecionar print() para arquivo tamb√©m\n",
    "class TeeOutput:\n",
    "    \"\"\"Redireciona tudo que seria printado para o terminal e tamb√©m para um arquivo.\"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        self.file = open(filepath, \"a\", encoding=\"utf-8\")\n",
    "        self.terminal = sys.stdout\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.file.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.file.flush()\n",
    "\n",
    "# Redirecionar todos os prints para outputs/\n",
    "output_filename = datetime.now().strftime(\"outputs/output_%Y-%m-%d_%H-%M-%S.txt\")\n",
    "sys.stdout = TeeOutput(output_filename)\n",
    "\n",
    "logging.info(\"üöÄ Logging inicializado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75daef6",
   "metadata": {},
   "source": [
    "## 1.2 S√©ries Hist√≥ricas ‚Äî API \n",
    "\n",
    "Foram coletadas s√©ries hist√≥ricas de 6 meses das a√ß√µes de empresas com participa√ß√£o relevante da Berkshire Hathaway.  \n",
    "Foram inclu√≠das as a√ß√µes: **Apple (AAPL)**, **Occidental Petroleum (OXY)** e **Coca-Cola (KO)**.  \n",
    "Esses dados incluem pre√ßo de abertura, fechamento, volume e varia√ß√£o di√°ria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "import time \n",
    "from datetime import datetime \n",
    "\n",
    "API_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "TICKERS = [\"AAPL\", \"OXY\", \"KO\"] \n",
    "dfs = [] \n",
    "\n",
    "logging.info(\"Iniciando coleta de dados da Alpha Vantage...\") \n",
    "\n",
    "for ticker in TICKERS: \n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={API_KEY}\" \n",
    "    logging.info(f\"Buscando dados para {ticker}...\") \n",
    "    r = requests.get(url) \n",
    "    time.sleep(10)\n",
    "    \n",
    "    try: \n",
    "        data = r.json().get(\"Time Series (Daily)\", {}) \n",
    "    except Exception as e: \n",
    "        logging.error(f\"Erro ao decodificar JSON para {ticker}: {e}\") \n",
    "        print(\"Resposta da API:\", r.text[:300]) \n",
    "        time.sleep(20) \n",
    "        continue \n",
    "    \n",
    "    if not data: \n",
    "        logging.warning(f\"Nenhum dado retornado para {ticker}.\") \n",
    "        continue \n",
    "    \n",
    "    df = pd.DataFrame(data).T \n",
    "    df.columns = [\"abertura\", \"maxima\", \"minima\", \"fechamento\", \"volume\"] \n",
    "    df.index = pd.to_datetime(df.index) \n",
    "    df = df[df.index >= pd.Timestamp.today() - pd.DateOffset(months=6)]\n",
    "    df = df.reset_index().rename(columns={\"index\": \"data\"}) \n",
    "    df[\"ticker\"] = ticker \n",
    "    dfs.append(df) \n",
    "    logging.info(f\"‚úÖ Dados coletados com sucesso para {ticker} ({len(df)} linhas).\") \n",
    "    time.sleep(15)  \n",
    "    \n",
    "    \n",
    "# Combinar tudo e salvar \n",
    "df_final = pd.concat(dfs, ignore_index=True) \n",
    "df_final.to_parquet(\"data_raw/stock_prices_raw.parquet\", index=False) \n",
    "logging.info(\"üíæ Dados salvos em data_raw/stock_prices_raw.parquet\") \n",
    "print(\"‚úÖ Coleta conclu√≠da com sucesso!\") \n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf87d3",
   "metadata": {},
   "source": [
    "## 1.3 Movimenta√ß√µes da Berkshire Hathaway ‚Äî Web Scraping (WhaleWisdom)\n",
    "\n",
    "Foi desenvolvido um scraper para coletar informa√ß√µes das movimenta√ß√µes trimestrais da Berkshire Hathaway a partir do site WhaleWisdom.       \n",
    "Foram extra√≠dos dados como:\n",
    "\n",
    "- Nome da empresa,\n",
    "\n",
    "- C√≥digo do ativo (Ticker),\n",
    "\n",
    "- Tipo de movimenta√ß√£o (compra/venda),\n",
    "\n",
    "- Percentual de varia√ß√£o na posi√ß√£o,\n",
    "\n",
    "- Data de atualiza√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0005c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import bs4 as BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4955b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"üîπ Iniciando o Edge WebDriver...\")\n",
    "\n",
    "path = r\"C:\\WebDrivers\\msedgedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--inprivate\")\n",
    "\n",
    "service = Service(executable_path=path)\n",
    "driver = webdriver.Edge(service=service, options=options)\n",
    "\n",
    "url = \"https://whalewisdom.com/filer/berkshire-hathaway-inc\"\n",
    "driver.get(url)\n",
    "logging.info(f\"Acessando p√°gina: {url}\")\n",
    "\n",
    "time.sleep(10)\n",
    "driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "holdings = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[2]/div/div[1]/div/div[2]/div/div[3]')\n",
    "holdings.click()\n",
    "print(\"‚úÖ Entrei no holdings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4c35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollBy(0, 200);\")\n",
    "print(\"Scroll inicial realizado para localizar footer...\")\n",
    "\n",
    "footer = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"v-data-footer\"))\n",
    ")\n",
    "itens_per_page = footer.find_element(By.CLASS_NAME, 'v-icon__svg')\n",
    "itens_per_page.click()\n",
    "print(\"Cliquei no seletor de itens por p√°gina\")\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0, 100);\")\n",
    "cem_input = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable(\n",
    "        (By.XPATH, \"//div[contains(@class, 'v-list-item__title') and normalize-space(text())='100']\")\n",
    "    )\n",
    ")\n",
    "driver.execute_script(\"arguments[0].click();\", cem_input)\n",
    "print(\"‚úÖ Selecionado 100 itens por p√°gina\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab78b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andr√©\\AppData\\Local\\Temp\\ipykernel_7612\\2426770513.py:21: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html)[0]\n"
     ]
    }
   ],
   "source": [
    "tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "print(f\"Quantidade de tabelas encontradas: {len(tables)}\")\n",
    "# Selecionar apenas as colunas desejadas\n",
    "colunas_desejadas = [\n",
    "    \"Stock\",\n",
    "    \"Shares Held or Principal Amt\",\n",
    "    \"Market Value\",\n",
    "    \"% of Portfolio\",\n",
    "    \"Previous % of Portfolio\",\n",
    "    \"Rank\",\n",
    "    \"Change in Shares\",\n",
    "    \"% Change\",\n",
    "    \"Qtr 1st Owned\",\n",
    "    \"Source Date\",\n",
    "    \"Date Reported\"\n",
    "]\n",
    "\n",
    "# Pegando a tabela espec√≠fica\n",
    "try:\n",
    "    html = tables[4].get_attribute(\"outerHTML\")\n",
    "    df = pd.read_html(html)[0]\n",
    "    logging.info(\"Colunas desejadas selecionadas\")\n",
    "    df = df[colunas_desejadas]\n",
    "    print(df.head())\n",
    "    df.to_parquet(\"data_raw/movimentacoesBerkshireHathaway_raw.parquet\", index=False)\n",
    "    logging.info(\"üíæ Dados salvos em data_raw/movimentacoesBerkshireHathaway_raw.parquet\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao processar ou salvar a tabela: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "print(\"üëã WebDriver finalizado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f4d5c",
   "metadata": {},
   "source": [
    "## 1.4 Not√≠cias sobre Warren Buffett e empresas investidas ‚Äî Web Scraping \n",
    "\n",
    "Foi desenvolvido um segundo scraper para coletar not√≠cias do site investing.com, relacionadas a Warren Buffett e suas empresas investidas.  \n",
    "Foram extra√≠dos, no m√≠nimo, 100 not√≠cias v√°lidas contendo:\n",
    "\n",
    "- T√≠tulo da not√≠cia,\n",
    "\n",
    "- Data/hora de publica√ß√£o,\n",
    "\n",
    "- URL da mat√©ria original,\n",
    "\n",
    "- Primeiro par√°grafo (lead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d257b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time, random\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONFIGURA√á√ïES ======\n",
    "search_terms = [\"Warren Buffett\", \"Apple\", \"Coca-Cola\", \"Occidental Petroleum\"]  # \"Berkshire Hathaway\"\n",
    "NEWS_PER_TERM = 30\n",
    "\n",
    "logging.info(\"üîß Configura√ß√µes carregadas:\")\n",
    "logging.info(f\"   - Termos de busca: {search_terms}\")\n",
    "logging.info(f\"   - Not√≠cias por termo: {NEWS_PER_TERM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebfe38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coletar_noticias(term, driver):\n",
    "    logging.info(f\"üì∞ Iniciando coleta de not√≠cias para '{term}'\")\n",
    "\n",
    "    urls = {\n",
    "        \"Warren Buffett\": \"https://www.investing.com/search/?q=Warren+Buffett&tab=news\",\n",
    "        \"Apple\": \"https://www.investing.com/search/?q=Apple&tab=news\",\n",
    "        \"Coca-Cola\": \"https://www.investing.com/search/?q=Coca-cola&tab=news\",\n",
    "        \"Occidental Petroleum\": \"https://www.investing.com/search/?q=Occidental%20Petroleum&tab=news\"\n",
    "    }\n",
    "\n",
    "    # Fecha o driver atual e reabre um novo\n",
    "    try:\n",
    "        pid = driver.service.process.pid\n",
    "        driver.quit()\n",
    "        time.sleep(1)\n",
    "        psutil.Process(pid).kill()\n",
    "        print(\"üßπ Processo do driver encerrado com seguran√ßa.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"‚ö†Ô∏è Erro ao encerrar driver: {e}\")\n",
    "\n",
    "    EDGE_DRIVER_PATH = r\"C:\\WebDrivers\\msedgedriver.exe\"\n",
    "\n",
    "    edge_options = Options()\n",
    "    edge_options.add_argument(\"--start-maximized\")\n",
    "    edge_options.add_argument(\"--disable-gpu\")\n",
    "    edge_options.add_argument(\"--inprivate\")\n",
    "\n",
    "    service = EdgeService(executable_path=EDGE_DRIVER_PATH)\n",
    "    driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "    # Abre a URL referente ao termo\n",
    "    if term in urls:\n",
    "        url = urls[term]\n",
    "        logging.info(f\"üåê Acessando URL de busca: {url}\")\n",
    "        driver.get(url)\n",
    "    else:\n",
    "        logging.warning(f\"‚ö†Ô∏è Termo '{term}' n√£o encontrado. Usando URL padr√£o (Occidental Petroleum).\")\n",
    "        driver.get(urls[\"Occidental Petroleum\"])\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Fecha pop-up de cookies\n",
    "    try:\n",
    "        cookie_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"onetrust-close-btn-container\"]/button'))\n",
    "        )\n",
    "        cookie_btn.click()\n",
    "        print(\"üç™ Cookies aceitos com sucesso.\")\n",
    "    except Exception:\n",
    "        logging.debug(\"Nenhum pop-up de cookies detectado.\")\n",
    "\n",
    "    news_list = []\n",
    "    collected = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Coleta de not√≠cias com logs\n",
    "    while collected < NEWS_PER_TERM:\n",
    "        articles = driver.find_elements(By.CLASS_NAME, 'articleItem')\n",
    "        for art in articles[collected:]:\n",
    "            try:\n",
    "                titulo = art.find_element(By.CLASS_NAME, 'title').text.strip()\n",
    "                url = art.find_element(By.CLASS_NAME, 'title').get_attribute(\"href\")\n",
    "                data = art.find_element(By.CLASS_NAME, 'date').text.strip()\n",
    "                lead = art.find_element(By.CLASS_NAME, 'js-news-item-content').text.strip() \n",
    "                \n",
    "                news_list.append({\n",
    "                    \"termo\": term,\n",
    "                    \"titulo\": titulo,\n",
    "                    \"url\": url,\n",
    "                    \"data\": data,\n",
    "                    \"lead\": lead\n",
    "                })\n",
    "                collected += 1\n",
    "\n",
    "                if collected % 5 == 0:\n",
    "                    print(f\"üóûÔ∏è {collected} not√≠cias coletadas at√© agora para '{term}'\")\n",
    "\n",
    "                if collected >= NEWS_PER_TERM:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"üîö Fim da p√°gina atingido ‚Äî nenhuma nova not√≠cia carregada.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    logging.info(f\"‚úÖ {len(news_list)} not√≠cias coletadas para '{term}'.\")\n",
    "    return news_list, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3986ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== EXECU√á√ÉO ======\n",
    "logging.info(\"üöÄ Iniciando processo de coleta de not√≠cias para todos os termos...\")\n",
    "todas_noticias = []\n",
    "\n",
    "for term in search_terms:\n",
    "    noticias, driver = coletar_noticias(term, driver)\n",
    "    todas_noticias.extend(noticias)\n",
    "    logging.info(f\"üì¶ {len(noticias)} not√≠cias adicionadas para '{term}'\")\n",
    "\n",
    "driver.quit()\n",
    "logging.info(\"üß© WebDriver encerrado ap√≥s coleta de todas as not√≠cias.\")\n",
    "\n",
    "df_news = pd.DataFrame(todas_noticias)\n",
    "file_path = \"data_raw/news_buffett_raw.parquet\"\n",
    "df_news.to_parquet(file_path, index=False)\n",
    "print(df_news.head())\n",
    "logging.info(f\"üíæ Total de {len(df_news)} not√≠cias salvas em {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0d5e5",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Transforma√ß√£o dos Dados\n",
    "\n",
    "Ap√≥s a coleta, foi realizada a padroniza√ß√£o e limpeza dos dados para garantir consist√™ncia e integridade.    \n",
    "As principais etapas inclu√≠ram:\n",
    "\n",
    "- Normaliza√ß√£o de formatos de data e hora;\n",
    "\n",
    "- Convers√£o de tipos num√©ricos e textuais;\n",
    "\n",
    "- Remo√ß√£o de registros duplicados;\n",
    "\n",
    "- Tratamento de valores ausentes;\n",
    "\n",
    "- Gera√ß√£o de chaves √∫nicas para integra√ß√£o entre datasets.\n",
    "\n",
    "## 2.1 Transforma√ß√£o dos dados das a√ß√£oes (1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f3fb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "logging.info(\"Tratamento dos dados stock_prices come√ßou.\")\n",
    "df = pd.read_parquet('data_raw/stock_prices_raw.parquet')\n",
    "\n",
    "\n",
    "# Remove o tempo e converte para formato dd/mm/yyyy\n",
    "df['data'] = pd.to_datetime(df['data']).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Criar coluna com o nome da empresa\n",
    "ticker_to_name = {\n",
    "    'AAPL': 'Apple Inc.',\n",
    "    'KO': 'Coca-Cola Co.',\n",
    "    'OXY': 'Occidental Petroleum'\n",
    "}\n",
    "df['empresa'] = df['ticker'].map(ticker_to_name)\n",
    "\n",
    "\n",
    "cols_float = ['abertura', 'maxima', 'minima', 'fechamento']\n",
    "df[cols_float] = df[cols_float].astype(float)\n",
    "\n",
    "df['volume'] = df['volume'].astype(int)\n",
    "\n",
    "# Adicionar coluna com valor total negociado\n",
    "df['valor_total_negociado'] = df['fechamento'] * df['volume']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_parquet('data_clean/stock_prices_clean.parquet', index=False)\n",
    "logging.info(\"Dados limpos salvos em data_clean/stock_prices_clean.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8931e3",
   "metadata": {},
   "source": [
    "## 2.2 Transforma√ß√£o dos dados das movimenta√ß√µes da Berkshire Hathaway (1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.info(\"Tratamento dos dados das movimenta√ß√µes da Berkshire\")\n",
    "\n",
    "df = pd.read_parquet('data_raw/movimentacoesBerkshireHathaway_raw.parquet')\n",
    "\n",
    "# Renomear colunas\n",
    "renomear = {\n",
    "    \"Stock\": \"A√ß√£o\",\n",
    "    \"Shares Held or Principal Amt\": \"A√ß√µes ou Valor Principal\",\n",
    "    \"Market Value\": \"Valor de Mercado\",\n",
    "    \"% of Portfolio\": \"% do Portf√≥lio\",\n",
    "    \"Previous % of Portfolio\": \"% Anterior do Portf√≥lio\",\n",
    "    \"Rank\": \"Posi√ß√£o\",\n",
    "    \"Change in Shares\": \"Mudan√ßa em A√ß√µes\",\n",
    "    \"% Change\": \"% de Mudan√ßa\",\n",
    "    \"Qtr 1st Owned\": \"Primeiro Trimestre Possu√≠do\",\n",
    "    \"Source Date\": \"Data da Fonte\",\n",
    "    \"Date Reported\": \"Data Reportada\"\n",
    "}\n",
    "df = df.rename(columns=renomear)\n",
    "logging.info(\"Traduzido as colunas\")\n",
    "\n",
    "# Ajustar datas\n",
    "for col in [\"Data da Fonte\", \"Data Reportada\"]:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Tratar coluna 'Mudan√ßa em A√ß√µes'\n",
    "df['Mudan√ßa em A√ß√µes'] = (\n",
    "    df['Mudan√ßa em A√ß√µes']\n",
    "    .replace({'No Change': 0, 'Change from Form 4 filing': 0})\n",
    ")\n",
    "df['Mudan√ßa em A√ß√µes'] = pd.to_numeric(df['Mudan√ßa em A√ß√µes'], errors='coerce')\n",
    "\n",
    "# Tratar coluna '% do Portf√≥lio'\n",
    "df['% do Portf√≥lio'] = df['% do Portf√≥lio'].fillna('0.00%')\n",
    "df['% do Portf√≥lio'] = df['% do Portf√≥lio'].replace({'%': ''}, regex=True).astype(float)\n",
    "\n",
    "# Tratar coluna '% Anterior do Portf√≥lio'\n",
    "df['% Anterior do Portf√≥lio'] = df['% Anterior do Portf√≥lio'].fillna('0.00%')\n",
    "df['% Anterior do Portf√≥lio'] = df['% Anterior do Portf√≥lio'].replace({'%': ''}, regex=True).astype(float)\n",
    "\n",
    "# Tratar coluna '% de Mudan√ßa' \n",
    "def tratar_percentual(valor):\n",
    "    if pd.isna(valor) or valor == '':\n",
    "        return 0.00\n",
    "    elif 'New' in str(valor):\n",
    "        return 100.00\n",
    "    else:\n",
    "        return float(str(valor).replace('%', '').strip())\n",
    "\n",
    "df['% de Mudan√ßa'] = df['% de Mudan√ßa'].apply(tratar_percentual)\n",
    "\n",
    "# Tratar coluna 'Posi√ß√£o'\n",
    "# Substituir 'Sold All' por 0 na coluna 'Posi√ß√£o'\n",
    "df['Posi√ß√£o'] = df['Posi√ß√£o'].replace('Sold All', 0)\n",
    "\n",
    "# Converter colunas num√©ricas\n",
    "numericas = [\n",
    "    \"A√ß√µes ou Valor Principal\", \"Valor de Mercado\", \"Posi√ß√£o\", \"Mudan√ßa em A√ß√µes\"\n",
    "]\n",
    "\n",
    "for col in numericas:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Substituir vazios/NaN por -1\n",
    "df['Posi√ß√£o'] = df['Posi√ß√£o'].fillna(-1)\n",
    "\n",
    "# üîß Garantir que todas as colunas num√©ricas sejam float64\n",
    "df[numericas] = df[numericas].astype('float64')\n",
    "\n",
    "# Agora salva sem erro\n",
    "df.to_parquet('data_clean/movimentacoesBerkshireHathaway_clean.parquet', index=False)\n",
    "\n",
    "logging.info(\"Dados da Berkshire tratados e salvos!\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584e800",
   "metadata": {},
   "source": [
    "## 2.3 Tranforma√ß√£o dos dados de not√≠cias (1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17754696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def tratar_data(valor):\n",
    "    agora = datetime.now()\n",
    "    \n",
    "    # Caso \"X minutes ago\"\n",
    "    if \"minute\" in valor:\n",
    "        return agora.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # Caso \"X hours ago\"\n",
    "    elif \"hour\" in valor:\n",
    "        match = re.search(r'(\\d+)', valor)\n",
    "        if match:\n",
    "            horas = int(match.group(1))\n",
    "            nova_data = agora - timedelta(hours=horas)\n",
    "            return nova_data.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            return agora.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # Caso data padr√£o (ex: \"Oct 15, 2025\")\n",
    "    else:\n",
    "        try:\n",
    "            data_formatada = pd.to_datetime(valor, format='%b %d, %Y')\n",
    "            return data_formatada.strftime('%d/%m/%Y')\n",
    "        except Exception:\n",
    "            logging.error(\"N√£o conseguiu converter a data\")\n",
    "            return None  # se n√£o conseguir converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b6bfd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logging.info(\"Tratamento dos dados de not√≠cias come√ßou.\")\n",
    "df = pd.read_parquet('data_raw/news_buffett_raw.parquet')\n",
    "\n",
    "\n",
    "df['data'] = df['data'].astype(str).apply(tratar_data)\n",
    "\n",
    "# Remover duplicatas\n",
    "df = df.drop_duplicates(subset=['titulo', 'url'], keep='first')\n",
    "\n",
    "\n",
    "df['lead'] = df['lead'].replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_parquet('data_clean/news_buffett_clean.parquet', index=False)\n",
    "logging.info(\"Dados limpos salvos em data_clean/news_buffett_clean.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d385e",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Considera√ß√µes Finais\n",
    "\n",
    "O pipeline desenvolvido demonstra a aplica√ß√£o pr√°tica de t√©cnicas de Web Mining e Web Scraping integradas a ETL anal√≠tico, proporcionando uma base consolidada para an√°lise de correla√ß√£o entre eventos do mercado financeiro e decis√µes de investimento da Berkshire Hathaway.\n",
    "\n",
    "A coleta automatizada e o tratamento dos dados permitiram a gera√ß√£o de um banco anal√≠tico reproduz√≠vel, que pode ser facilmente explorado em ferramentas SQL ou visualizado em dashboards, possibilitando estudos mais profundos sobre o comportamento do mercado diante das decis√µes de grandes investidores.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Arquivos Entregues\n",
    "\n",
    "- Notebook: pipeline_warren_buffett.ipynb\n",
    "\n",
    "- Dados tratados:\n",
    "\n",
    "    - stock_prices.parquet\n",
    "\n",
    "    - portfolio_movements.parquet\n",
    "\n",
    "    - news.parquet\n",
    "\n",
    "- Arquivo de depend√™ncias: requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6404952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
